{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T14:45:55.473869Z",
     "start_time": "2019-02-05T14:45:54.555438Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fastai import *\n",
    "from fastai.text import * \n",
    "import difflib\n",
    "from tqdm import tqdm\n",
    "PATH = Path('./data/')\n",
    "import concurrent.futures\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import Button, HBox, VBox\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "from LabelMyTextWidget import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T14:50:46.425141Z",
     "start_time": "2019-02-05T14:50:46.394635Z"
    }
   },
   "outputs": [],
   "source": [
    "df_source = pd.read_csv(PATH/'perf_db.csv', index_col=0)\n",
    "df_generated = pd.read_csv(PATH/'next-generated-batch-dataset-1.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing of generated quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data has been tokenized, there are post-processing steps to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T15:06:28.044855Z",
     "start_time": "2019-02-05T15:06:28.030570Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_quotes_refactoring(quote: str) -> str:\n",
    "    res_string = \"\"\n",
    "    ponc = ['?', '.', ',', ';', '\\n', ':', ':', '-', '(', ')', \"n't\", \"'re\", \"'s\", \"'m\", \"'d\", '!']\n",
    "    quotes_word_list = quote.split(' ')\n",
    "    next_maj = False\n",
    "    next_all_caps = False\n",
    "    \n",
    "    for i, w in enumerate(quotes_word_list):\n",
    "        if w == '':\n",
    "            continue\n",
    "            \n",
    "        if len(res_string) == 0:\n",
    "            w.capitalize()\n",
    "            \n",
    "        \n",
    "        if w == 'xxmaj':\n",
    "            next_maj = True\n",
    "            continue\n",
    "            \n",
    "        elif next_maj:\n",
    "            w = w.capitalize()\n",
    "            next_maj = False\n",
    "            \n",
    "        if w == 'xxup':\n",
    "            next_all_caps = True\n",
    "            continue\n",
    "        elif next_all_caps:\n",
    "            w = w.upper()\n",
    "            next_all_caps = False    \n",
    "            \n",
    "        if w == '.':\n",
    "            next_all_caps = True\n",
    "        \n",
    "        if w == 'i':\n",
    "            w = w.capitalize()\n",
    "            \n",
    "        if w not in ponc and len(res_string) > 0:\n",
    "            if res_string[-1] != '-':\n",
    "                res_string = res_string + \" \"\n",
    "        \n",
    "        res_string = res_string + w\n",
    "            \n",
    "    return res_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T14:50:51.288114Z",
     "start_time": "2019-02-05T14:50:51.273866Z"
    }
   },
   "outputs": [],
   "source": [
    "s = df_generated.text.iloc[0]\n",
    "print(s), print(process_quotes_refactoring(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T15:06:32.400821Z",
     "start_time": "2019-02-05T15:06:32.303058Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new_quotes = pd.DataFrame(columns=['text'])\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(df_generated))):\n",
    "    q = df_generated.text.iloc[i]\n",
    "    q_corrected = process_quotes_refactoring(q)\n",
    "    \n",
    "    df_new_quotes.loc[i] = q_corrected\n",
    "    \n",
    "assert(len(df_new_quotes) == len(df_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T15:06:34.348503Z",
     "start_time": "2019-02-05T15:06:34.330200Z"
    }
   },
   "outputs": [],
   "source": [
    "df_generated = df_new_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elimination of plagia: delete generated quotes too close to original corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T14:53:17.549011Z",
     "start_time": "2019-02-05T14:53:17.532526Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_quote_similarity(df_to_check, df_source, threshold=0.80, begin_indice=0, end_indice=-1):\n",
    "    to_remove = np.array([])\n",
    "    if end_indice == 0 or end_indice > len(df_to_check) or begin_indice >= end_indice:\n",
    "        begin_indice = 0\n",
    "        end_indice = len(df_to_check)-1\n",
    "        \n",
    "    for i in tqdm(range(begin_indice, end_indice+1)):\n",
    "        q = df_to_check.text.iloc[i]\n",
    "        tab_sim = np.array([])\n",
    "        for j in range(len(df_source)):\n",
    "            t = df_source.text.iloc[j]\n",
    "            sim = difflib.SequenceMatcher(None, q, t).ratio()\n",
    "            tab_sim = np.append(tab_sim, sim)\n",
    "\n",
    "        most_sim = np.argmax(tab_sim)\n",
    "        \n",
    "        if tab_sim[most_sim] > threshold:\n",
    "            print(f'Threshold alert: {q}\\n Most sim : {df_source.text.iloc[most_sim]},\\n indice {tab_sim[most_sim]}\\n\\n\\n')\n",
    "            to_remove = np.append(to_remove, i)    \n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T14:51:02.795124Z",
     "start_time": "2019-02-05T14:51:02.782425Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = df_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will execute the check_quote_similarity function using multi-threading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T14:54:12.008205Z",
     "start_time": "2019-02-05T14:53:19.424734Z"
    }
   },
   "outputs": [],
   "source": [
    "n_task = 6\n",
    "futures = []\n",
    "res = np.array([])\n",
    "with ProcessPoolExecutor(max_workers=n_task) as ex:\n",
    "    number_by_worker = len(df_test) // n_task\n",
    "    print(f'Number by worker : {number_by_worker}')\n",
    "    \n",
    "    for i in range(n_task):\n",
    "        begin_indice = number_by_worker*i\n",
    "        end_indice = (number_by_worker*i) + number_by_worker-1\n",
    "        \n",
    "        if i == n_task - 1:\n",
    "            end_indice = len(df_test)-1\n",
    "        print(f'{i}: will launch work from {begin_indice} to {end_indice}')\n",
    "        \n",
    "        \n",
    "        futures.append(ex.submit(check_quote_similarity,df_test,df_source, begin_indice=begin_indice, end_indice=end_indice))\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        return_future = future.result()\n",
    "        print(return_future)\n",
    "        #res.append(return_future)\n",
    "        res = np.concatenate((res, return_future))\n",
    "        print(f'Res : {res}')\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T14:54:14.157372Z",
     "start_time": "2019-02-05T14:54:14.140379Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.loc[res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quotes which are too similar are droped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T14:54:18.061264Z",
     "start_time": "2019-02-05T14:54:18.045889Z"
    }
   },
   "outputs": [],
   "source": [
    "df_q = df_test.drop(res)\n",
    "len(df_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T14:54:20.469255Z",
     "start_time": "2019-02-05T14:54:20.450827Z"
    }
   },
   "outputs": [],
   "source": [
    "df_q['label'] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering of generated quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step of DeepGuru will be to use a second Neural Network to select the best quotes.\n",
    "However, I have still work to do, to make it effective.\n",
    "For the time being, there is a manual step of tagging the best quotes. \n",
    "This will soon be automated.\n",
    "\n",
    "The widget from https://github.com/tchambon/LabelMyTextWidget will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T14:54:23.289752Z",
     "start_time": "2019-02-05T14:54:23.246882Z"
    }
   },
   "outputs": [],
   "source": [
    "w = LabelMyTextWidget(df_q, 'text', ['ko', 'ok'], [0, 1], 'label', randomize=False)\n",
    "w.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T15:06:02.424938Z",
     "start_time": "2019-02-05T15:06:02.409230Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df_q[df_q.label==0]), len(df_q[df_q.label==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T15:10:07.450808Z",
     "start_time": "2019-02-05T15:10:07.437826Z"
    }
   },
   "outputs": [],
   "source": [
    "df_publish = df_q[df_q.label == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T15:11:21.513318Z",
     "start_time": "2019-02-05T15:11:21.497067Z"
    }
   },
   "outputs": [],
   "source": [
    "df_publish.to_csv(PATH/'next_batch_to_publish.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T15:11:23.554565Z",
     "start_time": "2019-02-05T15:11:23.536429Z"
    }
   },
   "outputs": [],
   "source": [
    "data_processed = pd.read_csv('data/next_batch_to_publish.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T15:11:25.557441Z",
     "start_time": "2019-02-05T15:11:25.537819Z"
    }
   },
   "outputs": [],
   "source": [
    "list(data_processed.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
